# yolov5-带-SE

## 注意力机制概述

的概念**注意机制**源于研究**人类视觉**在认知科学中。由于信息处理的瓶颈，人类选择性地关注所有可用信息的一部分，而忽略其他可见信息。

这种能力源于人类视网膜的不同部分具有不同的信息处理能力——即不同部分具有不同的敏锐度。这**中央凹**人类视网膜的敏锐度最高。为了合理利用有限的视觉信息处理资源，人类需要选择视觉区域的特定部分并对其进行关注。

例如，当人们在电脑屏幕上观看电影时，他们会关注和处理电脑屏幕范围内的视觉，而忽略电脑屏幕之外（如键盘、电脑背景等）的视觉。

## 神经网络中的注意力机制

在神经网络中引入注意力机制的方法有很多。服用**convolutional neural networks (CNNs)**例如，您可以：

-   **空间注意力**：引入空间维度的注意力机制
-   **渠道关注**：在通道维度引入注意力机制
-   **混合注意力**：同时添加空间和通道维度的注意力机制

## SE (Squeeze-and-Excitation) Block Structure

![SE Block Structure](https://user-images.githubusercontent.com/120677884/207951728-b85f94dd-e95f-42c9-a681-5b3293f15b8d.png)

### 1. 挤压

将通道上的整个空间特征编码为全局特征。用途**全局平均池化** to compress the two-dimensional feature (H×W) of each channel into a real number.

### 2. 励磁

动态生成每个特征通道的权重值。它使用**two fully connected layers**形成瓶颈结构来对通道之间的相关性进行建模，并输出与输入特征相同数量的权重值。

### 3. 规模

通过激励学到的归一化权重被加权到每个通道的特征。
